<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Iris Voice Assistant</title>
  <style>
    html, body {
      margin: 0;
      padding: 0;
      background-color: black;
      color: white;
      font-family: sans-serif;
      height: 100%;
      overflow: hidden;
    }
    #avatar {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100%;
      position: relative;
    }
    video {
      max-height: 100vh;
    }
    #status {
      position: absolute;
      bottom: 20px;
      left: 0;
      right: 0;
      text-align: center;
      padding: 8px;
      color: rgba(255,255,255,0.7);
      font-size: 14px;
      transition: opacity 0.3s;
    }
    .listening #status {
      color: rgba(100,255,100,0.9);
    }
    .processing #status {
      color: rgba(255,200,0,0.9);
    }
    .speaking #status {
      color: rgba(100,200,255,0.9);
    }
    .error #status {
      color: rgba(255,100,100,0.9);
    }
  </style>
</head>
<body>
  <!-- More visible test button with brighter colors -->
  <button id="testSpeech" style="position: absolute; top: 20px; right: 20px; z-index: 2000; padding: 15px; background-color: red; color: white; font-weight: bold; border: none; border-radius: 8px; cursor: pointer; box-shadow: 0 4px 8px rgba(0,0,0,0.2);">
    TEST SPEECH
  </button>
  <div id="avatar">
    <video autoplay muted loop id="avatarVideo">
      <source src="nova-avatar-loop.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    <div id="status">Initializing...</div>
  </div>
  <script>
    const synth = window.speechSynthesis;
    let recognition;
    let voices = [];
    const statusEl = document.getElementById('status');
    const avatarEl = document.getElementById('avatar');

        // Server URL - change to your actual endpoint
    const SERVER_URL = "https://irisnova-prototype.onrender.com";
    
    // Add this test button handler
document.getElementById('testSpeech').addEventListener('click', function() {
  console.log("Test speech button clicked");
  const utterance = new SpeechSynthesisUtterance("This is a test of the speech synthesis system.");
  window.speechSynthesis.speak(utterance);
    });
    
    // Set status with visual feedback
    function setStatus(message, state = '') {
      console.log(`Status: ${message} (${state})`);
      statusEl.textContent = message;
      avatarEl.className = state;
    }
    // Ensure voices are loaded
    function loadVoices() {
      return new Promise((resolve) => {
        voices = synth.getVoices();
        if (voices.length > 0) {
          console.log(`Loaded ${voices.length} voices`);
          resolve(voices);
        } else {
          synth.onvoiceschanged = () => {
            voices = synth.getVoices();
            console.log(`Loaded ${voices.length} voices after change`);
            resolve(voices);
          };
          
          // Safety timeout
          setTimeout(() => {
            if (voices.length === 0) {
              console.warn("No voices loaded, using default");
              resolve([]);
            }
          }, 1000);
        }
      });
    }
    // Speak text with error handling
    async function speak(text) {
      try {
        console.log(`Speaking: "${text}"`);
        
        // Cancel any ongoing speech
        if (synth.speaking) {
          console.log("Cancelling previous speech");
          synth.cancel();
        }
        
        // Wait for voices to load if they haven't already
        if (voices.length === 0) {
          await loadVoices();
        }
        
        setStatus('Iris is speaking...', 'speaking');
        
        return new Promise((resolve) => {
          const utterance = new SpeechSynthesisUtterance(text);
          
          // Try to find an English female voice, fall back to any English voice
          const englishFemaleVoice = voices.find(voice => 
            voice.name.includes('Female') && voice.lang.includes('en')
          );
          const englishVoice = voices.find(voice => voice.lang.includes('en'));
          
          utterance.voice = englishFemaleVoice || englishVoice || (voices.length > 0 ? voices[0] : null);
          
          if (utterance.voice) {
            console.log(`Using voice: ${utterance.voice.name}`);
          } else {
            console.warn("No voice selected, using browser default");
          }
          
          utterance.rate = 1.0;
          utterance.pitch = 1.0;
          utterance.volume = 1.0;
          
          utterance.onend = () => {
            console.log("Speech finished");
            setStatus('Listening...', 'listening');
            resolve();
          };
          
          utterance.onerror = (err) => {
            console.error('Speech synthesis error:', err);
            setStatus('Error speaking', 'error');
            resolve();
          };
          
          synth.speak(utterance);
          
          // Safety timeout in case onend doesn't fire
          setTimeout(() => {
            if (synth.speaking) {
              console.warn("Speech didn't finish naturally, forcing resolve");
              setStatus('Listening...', 'listening');
              resolve();
            }
          }, text.length * 100); // Rough estimate: 100ms per character
        });
      } catch (error) {
        console.error('Speak error:', error);
        setStatus('Error speaking', 'error');
      }
    }
    // Check server health
    async function checkServerHealth() {
      try {
        const response = await fetch(`${SERVER_URL}/`);
        if (response.ok) {
          console.log('Server is healthy');
          return true;
        } else {
          console.error('Server returned error:', await response.text());
          return false;
        }
      } catch (error) {
        console.error('Server health check failed:', error);
        return false;
      }
    }
    // Initialize speech recognition
    function startListening() {
      try {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        recognition = new SpeechRecognition();
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = 'en-US';
        recognition.onstart = function() {
          setStatus('Listening...', 'listening');
        };
        recognition.onresult = async function(event) {
          try {
            const userInput = event.results[0][0].transcript;
            console.log('User said:', userInput);
            
            setStatus('Processing...', 'processing');
            
            const response = await fetch(`${SERVER_URL}/query`, {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ query: userInput })
            });
            
            if (!response.ok) {
              throw new Error(`Server returned ${response.status}: ${response.statusText}`);
            }
            
            const data = await response.json();
            console.log("Nova response:", data);
            
            if (data && data.reply) {
              // Force a slight delay to ensure previous utterances are canceled
              setTimeout(() => speak(data.reply), 100);
            } else {
              throw new Error('Empty response from server');
            }
          } catch (error) {
            console.error("Processing error:", error);
            setStatus('Error processing', 'error');
            setTimeout(() => {
              speak("I'm sorry, I had trouble connecting. Please try again.");
            }, 1000);
          }
        };
        recognition.onerror = function(event) {
          console.error("Speech recognition error:", event.error);
          setStatus(`Recognition error: ${event.error}`, 'error');
          setTimeout(startListening, 1000);
        };
        recognition.onend = function() {
          console.log("Recognition ended, restarting soon");
          setTimeout(startListening, 500);
        };
        recognition.start();
        console.log("Started listening");
      } catch (error) {
        console.error("Failed to start speech recognition:", error);
        setStatus('Speech recognition not available', 'error');
      }
    }
    // Initialize the application
    async function initApp() {
      try {
        console.log("Initializing app...");
        setStatus('Loading voices...', '');
        await loadVoices();
        
        setStatus('Checking server...', '');
        const serverOk = await checkServerHealth();
        
        if (serverOk) {
          await speak("Hi there, how can I help today?");
          startListening();
        } else {
          setStatus('Server unavailable', 'error');
          await speak("I'm sorry, I'm having trouble connecting to my brain. Please try again later.");
        }
        
        // Set inactivity timer
        let silenceTimer = setTimeout(resetNova, 180000); // 3 minutes
        
        function resetNova() {
          speak("I haven't heard anything else from you, I hope I helped. Let me know if I can do anything else for you.");
          setTimeout(() => location.reload(), 5000);
        }
        
        document.addEventListener("click", () => {
          clearTimeout(silenceTimer);
          silenceTimer = setTimeout(resetNova, 180000);
        });
      } catch (error) {
        console.error("Initialization error:", error);
        setStatus('Failed to initialize', 'error');
      }
    }
    window.onload = initApp;
  </script>
</body>
</html>
