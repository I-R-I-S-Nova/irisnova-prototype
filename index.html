<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Iris Voice Assistant</title>
  <style>
    html, body {
      margin: 0;
      padding: 0;
      background-color: black;
      color: white;
      font-family: sans-serif;
      height: 100%;
      overflow: hidden;
    }
    #avatar {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100%;
      position: relative;
    }
    video {
      max-height: 100vh;
    }
    #status {
      position: absolute;
      bottom: 20px;
      left: 0;
      right: 0;
      text-align: center;
      padding: 8px;
      color: rgba(255,255,255,0.7);
      font-size: 14px;
      transition: opacity 0.3s;
    }
    .listening #status {
      color: rgba(100,255,100,0.9);
    }
    .processing #status {
      color: rgba(255,200,0,0.9);
    }
    .speaking #status {
      color: rgba(100,200,255,0.9);
    }
    .error #status {
      color: rgba(255,100,100,0.9);
    }
    #startButton {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      padding: 20px 40px;
      font-size: 24px;
      background-color: #4CAF50;
      color: white;
      border: none;
      border-radius: 50px;
      cursor: pointer;
      z-index: 1000;
      box-shadow: 0 8px 16px rgba(0,0,0,0.3);
    }
    #transcript {
      position: absolute;
      bottom: 50px;
      left: 0;
      right: 0;
      text-align: center;
      padding: 10px;
      color: white;
      background-color: rgba(0,0,0,0.5);
      font-size: 16px;
    }
    #debugPanel {
      position: absolute;
      bottom: 10px;
      right: 10px;
      background-color: rgba(0,0,0,0.7);
      padding: 10px;
      border-radius: 5px;
      font-size: 12px;
      max-width: 300px;
      display: none;
    }
  </style>
</head>
<body>
  <div id="avatar">
    <video autoplay muted loop id="avatarVideo">
      <source src="nova-avatar-loop.mp4" type="video/mp4" />
      Your browser does not support the video tag.
    </video>
    <div id="status">Press Start to activate Iris</div>
    <div id="transcript"></div>
    <button id="startButton">Start Iris</button>
    <div id="debugPanel"></div>
  </div>
  <script>
    // Check for speech synthesis support
    console.log("Speech synthesis available:", 'speechSynthesis' in window);
    if ('speechSynthesis' in window) {
      window.speechSynthesis.cancel(); // Reset any existing speech
      console.log("Speech synthesis reset");
    }
    
    // Server URL - change to your actual endpoint
    const SERVER_URL = "https://irisnova-prototype.onrender.com";
    
    // Function to speak Dialogflow responses
    function speakDialogflowResponse(responseObject) {
      try {
        if (responseObject && responseObject.reply) {
          console.log("Speaking response:", responseObject.reply);
          
          // Cancel any ongoing speech
          window.speechSynthesis.cancel();
          
          const utterance = new SpeechSynthesisUtterance(responseObject.reply);
          
          // Try to use a female voice if available
          const voices = window.speechSynthesis.getVoices();
          const femaleVoice = voices.find(v => 
            (v.name.toLowerCase().includes('female') || 
             v.name.toLowerCase().includes('samantha')) && 
            v.lang.toLowerCase().includes('en')
          );
          
          if (femaleVoice) {
            console.log("Using voice:", femaleVoice.name);
            utterance.voice = femaleVoice;
          }
          
          // Start speaking
          window.speechSynthesis.speak(utterance);
          return true;
        } else {
          console.error("Invalid response object:", responseObject);
          return false;
        }
      } catch (error) {
        console.error("Speech synthesis error:", error);
        return false;
      }
    }
    
    // Direct speech function that uses the absolutely simplest approach
    function basicSpeak(text) {
      try {
        console.log("BASIC SPEAK:", text);
        
        // Create utterance with minimal options
        const utterance = new SpeechSynthesisUtterance(text);
        
        // Log when speech starts and ends
        utterance.onstart = () => console.log("BASIC SPEAK STARTED");
        utterance.onend = () => console.log("BASIC SPEAK ENDED");
        utterance.onerror = (e) => console.log("BASIC SPEAK ERROR:", e.error);
        
        // Speak immediately
        speechSynthesis.cancel(); // Cancel any existing speech
        speechSynthesis.speak(utterance);
        
        return true;
      } catch (e) {
        console.error("BASIC SPEAK FAILED:", e);
        return false;
      }
    }
    
    // Test speech function for console testing
    function testSpeech() {
      console.log("TEST SPEECH ACTIVATED");
      
      try {
        const text = "This is a basic speech test. Can you hear me now?";
        console.log("Speaking:", text);
        
        // Create utterance with minimal options
        const utterance = new SpeechSynthesisUtterance(text);
        
        // Log when speech starts and ends
        utterance.onstart = () => console.log("SPEECH STARTED");
        utterance.onend = () => console.log("SPEECH ENDED");
        utterance.onerror = (e) => console.log("SPEECH ERROR:", e.error);
        
        // Speak immediately
        window.speechSynthesis.cancel(); // Cancel any existing speech
        window.speechSynthesis.speak(utterance);
        
        console.log("Speech initiated");
        return true;
      } catch (e) {
        console.error("SPEECH TEST FAILED:", e);
        return false;
      }
    }
    
    // Alternative speech synthesis using audio element
    function audioSpeak(text) {
      try {
        console.log("AUDIO SPEAK:", text);
        
        // Create audio element
        const audio = new Audio();
        
        // Encode text for URL (limited to short phrases due to URL length limits)
        const encodedText = encodeURIComponent(text.substring(0, 100));
        
        // Set source to text-to-speech service
        audio.src = `https://translate.google.com/translate_tts?ie=UTF-8&tl=en-US&client=tw-ob&q=${encodedText}`;
        
        // Add error handling
        audio.onerror = (e) => {
          console.error("Audio speak error:", e);
        };
        
        // Play the audio
        audio.play().then(() => {
          console.log("Audio playback started");
        }).catch(e => {
          console.error("Audio playback error:", e);
        });
        
        return true;
      } catch (e) {
        console.error("AUDIO SPEAK FAILED:", e);
        return false;
      }
    }
    
    // Make these available globally
    window.speakDialogflowResponse = speakDialogflowResponse;
    window.basicSpeak = basicSpeak;
    window.testSpeech = testSpeech;
    window.audioSpeak = audioSpeak;
    
    // DOM Elements
    const statusEl = document.getElementById('status');
    const avatarEl = document.getElementById('avatar');
    const startButton = document.getElementById('startButton');
    const transcriptEl = document.getElementById('transcript');
    const debugPanel = document.getElementById('debugPanel');
    
    // Speech services
    const synth = window.speechSynthesis;
    let recognition = null;
    let voices = [];
    let selectedVoice = null;
    let isListening = false;
    let initialized = false;
    
    // Show debug information
    function debug(message) {
      console.log(message);
      debugPanel.style.display = 'block';
      debugPanel.innerHTML += message + '<br>';
      
      // Keep only the last 10 lines
      const lines = debugPanel.innerHTML.split('<br>');
      if (lines.length > 10) {
        debugPanel.innerHTML = lines.slice(lines.length - 10).join('<br>');
      }
    }
    
    // Set status with visual feedback
    function setStatus(message, state = '') {
      debug(`Status: ${message} (${state})`);
      statusEl.textContent = message;
      avatarEl.className = state;
    }
    
    // Initialize voices
    function initVoices() {
      return new Promise((resolve) => {
        function loadVoices() {
          voices = synth.getVoices();
          debug(`Loaded ${voices.length} voices`);
          
          // Find female English voice
          selectedVoice = voices.find(v => 
            v.name.toLowerCase().includes('female') && 
            v.lang.toLowerCase().includes('en')
          );
          
          // Try common female names
          if (!selectedVoice) {
            const femaleNames = ['samantha', 'victoria', 'karen', 'lisa', 'catherine', 'elizabeth'];
            selectedVoice = voices.find(v => 
              femaleNames.some(name => v.name.toLowerCase().includes(name)) && 
              v.lang.toLowerCase().includes('en')
            );
          }
          
          // Fallback to any English voice
          if (!selectedVoice) {
            selectedVoice = voices.find(v => v.lang.toLowerCase().includes('en'));
          }
          
          if (selectedVoice) {
            debug(`Selected voice: ${selectedVoice.name} (${selectedVoice.lang})`);
          } else {
            debug("No suitable voice found, using default");
          }
          
          resolve(selectedVoice);
        }
        
        // If voices are already loaded
        if (synth.getVoices().length > 0) {
          loadVoices();
        } else {
          // Wait for voices to load
          synth.onvoiceschanged = loadVoices;
          
          // Fallback in case onvoiceschanged doesn't fire
          setTimeout(() => {
            if (!selectedVoice && synth.getVoices().length > 0) {
              loadVoices();
            } else if (!selectedVoice) {
              debug("No voices loaded within timeout");
              resolve(null);
            }
          }, 1000);
        }
      });
    }
    
    // Speak text with better error handling
    function speak(text) {
      return new Promise((resolve) => {
        try {
          debug(`Speaking: "${text}"`);
          
          // Cancel any ongoing speech
          if (synth.speaking) {
            debug("Cancelling previous speech");
            synth.cancel();
          }
          
          const utterance = new SpeechSynthesisUtterance(text);
          
          // Use selected voice if available
          if (selectedVoice) {
            utterance.voice = selectedVoice;
          }
          
          // Adjust for more feminine sound
          utterance.pitch = 1.2;
          utterance.rate = 1.0;
          
          // Set up event handlers
          utterance.onstart = () => {
            debug("Speech started");
          };
          
          utterance.onend = () => {
            debug("Speech ended naturally");
            setStatus('Listening...', 'listening');
            resolve();
          };
          
          utterance.onerror = (err) => {
            debug(`Speech error: ${err.error}`);
            setStatus('Speech error', 'error');
            resolve();
          };
          
          // Start speaking
          setStatus('Iris is speaking...', 'speaking');
          synth.speak(utterance);
          
          // Safety timeout in case onend doesn't fire
          const timeout = text.length * 100 + 3000; // ~100ms per character + 3s base
          setTimeout(() => {
            if (synth.speaking) {
              debug(`Speech timeout after ${timeout}ms`);
              setStatus('Listening...', 'listening');
              resolve();
            }
          }, timeout);
        } catch (error) {
          debug(`Speak error: ${error.message}`);
          setStatus('Speech system error', 'error');
          resolve();
        }
      });
    }
    
    // Process user input with Dialogflow
    async function processInput(userInput) {
      try {
        // Update UI
        transcriptEl.textContent = `You: ${userInput}`;
        setStatus('Processing...', 'processing');
        
        // Log the input
        debug(`Processing input: "${userInput}"`);
        
        // Send to server
        const response = await fetch(`${SERVER_URL}/query`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ query: userInput })
        });
        
        // Check for errors
        if (!response.ok) {
          throw new Error(`Server returned ${response.status}: ${response.statusText}`);
        }
        
        // Parse response
        const data = await response.json();
        debug(`Response: ${JSON.stringify(data)}`);
        
        // Check for valid response
        if (data && data.reply) {
          // Update transcript
          transcriptEl.textContent += `\nIris: ${data.reply}`;
          
          // Try all possible approaches one after another
          debug("Attempting speech with multiple methods");
          
          try {
            // Method 1: Basic speak
            debug("Trying method 1: basicSpeak");
            basicSpeak(data.reply);
          } catch (e) {
            debug(`Method 1 failed: ${e.message}`);
          }
          
          try {
            // Method 2: Original DialogflowResponse
            debug("Trying method 2: speakDialogflowResponse");
            speakDialogflowResponse(data);
          } catch (e) {
            debug(`Method 2 failed: ${e.message}`);
          }
          
          try {
            // Method 3: Audio method
            debug("Trying method 3: audioSpeak");
            audioSpeak(data.reply);
          } catch (e) {
            debug(`Method 3 failed: ${e.message}`);
          }
          
          // Set status
          setStatus('Iris is speaking...', 'speaking');
          
          // Wait a reasonable time
          const waitTime = data.reply.length * 90 + 1000;
          debug(`Waiting ${waitTime}ms for speech to complete`);
          await new Promise(resolve => setTimeout(resolve, waitTime));
          
          // Set status back to listening
          setStatus('Listening...', 'listening');
        } else {
          throw new Error('Empty or invalid response from server');
        }
      } catch (error) {
        debug(`Process error: ${error.message}`);
        setStatus('Error processing request', 'error');
        
        // Try to speak the error message
        try {
          basicSpeak("I'm sorry, I encountered an error. Please try again.");
        } catch (e) {
          debug(`Error speaking error message: ${e.message}`);
        }
      } finally {
        // Restart listening if needed
        if (isListening && !recognition.recognizing) {
          startListening();
        }
      }
    }
    
    // Start speech recognition
    function startListening() {
      try {
        // Create recognition object if needed
        if (!recognition) {
          const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
          if (!SpeechRecognition) {
            throw new Error("Speech recognition not supported in this browser");
          }
          
          recognition = new SpeechRecognition();
          recognition.continuous = false;
          recognition.interimResults = false;
          recognition.lang = 'en-US';
          
          recognition.onstart = function() {
            debug("Recognition started");
            recognition.recognizing = true;
            setStatus('Listening...', 'listening');
          };
          
          recognition.onresult = async function(event) {
            recognition.recognizing = false;
            const userInput = event.results[0][0].transcript;
            debug(`Recognized: "${userInput}" (confidence: ${event.results[0][0].confidence.toFixed(2)})`);
            processInput(userInput);
          };
          
          recognition.onerror = function(event) {
            recognition.recognizing = false;
            debug(`Recognition error: ${event.error}`);
            
            if (event.error === 'no-speech') {
              debug("No speech detected, restarting...");
              setStatus('Waiting for speech...', 'listening');
              setTimeout(startListening, 1000);
            } else if (event.error === 'aborted') {
              debug("Recognition aborted");
            } else {
              setStatus(`Recognition error: ${event.error}`, 'error');
              setTimeout(startListening, 3000);
            }
          };
          
          recognition.onend = function() {
            recognition.recognizing = false;
            debug("Recognition ended");
            
            // Only restart if we're still supposed to be listening
            if (isListening) {
              debug("Restarting recognition...");
              setTimeout(startListening, 500);
            }
          };
        }
        
        // Stop any existing recognition
        try {
          if (recognition.recognizing) {
            recognition.abort();
          }
        } catch (e) {
          debug(`Error stopping recognition: ${e.message}`);
        }
        
        // Start recognition
        recognition.start();
        recognition.recognizing = true;
        
      } catch (error) {
        debug(`Start listening error: ${error.message}`);
        setStatus('Failed to start speech recognition', 'error');
      }
    }
    
    // Initialize the app
    async function initApp() {
      try {
        if (initialized) return;
        
        // Hide start button
        startButton.style.display = 'none';
        
        // Set initial status
        setStatus('Initializing...', '');
        
        // Initialize voices
        debug("Initializing voices...");
        await initVoices();
        
        // Welcome message
        debug("Speaking welcome message...");
        
        // Try multiple speech methods for welcome message
        try {
          basicSpeak("Hi there, I'm Iris. How can I help you today?");
        } catch (e) {
          debug(`Basic speak welcome failed: ${e.message}`);
        }
        
        try {
          speakDialogflowResponse({reply: "Hi there, I'm Iris. How can I help you today?"});
        } catch (e) {
          debug(`Dialogflow speak welcome failed: ${e.message}`);
        }
        
        setStatus('Iris is speaking...', 'speaking');
        
        // Wait a moment for the speech to complete
        await new Promise(resolve => setTimeout(resolve, 3000));
        
        // Start listening
        debug("Starting speech recognition...");
        isListening = true;
        initialized = true;
        startListening();
        
      } catch (error) {
        debug(`Initialization error: ${error.message}`);
        setStatus('Failed to initialize', 'error');
      }
    }
    
    // Start button click handler
    startButton.addEventListener('click', initApp);
    
    // Debug keyboard shortcuts
    document.addEventListener('keydown', function(e) {
      // Press 'D' to toggle debug panel
      if (e.key === 'd' || e.key === 'D') {
        debugPanel.style.display = debugPanel.style.display === 'none' ? 'block' : 'none';
      }
      
      // Press 'T' to test speech
      if (e.key === 't' || e.key === 'T') {
        debug("Test speech triggered");
        testSpeech();
      }
      
      // Press 'R' to restart listening
      if (e.key === 'r' || e.key === 'R' && initialized) {
        startListening();
      }
    });
    
    // Global access for console debugging
    window.debugIris = {
      speak: speak,
      startListening: startListening,
      processInput: processInput,
      init: initApp,
      speakDialogflow: speakDialogflowResponse,
      testSpeech: testSpeech,
      basicSpeak: basicSpeak,
      audioSpeak: audioSpeak
    };
  </script>
</body>
</html>
